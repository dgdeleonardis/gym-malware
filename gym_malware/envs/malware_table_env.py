import random
import gym
from collections import OrderedDict
from gym import error, spaces, utils
from gym.utils import seeding
import hashlib
import os

import json
import csv

import numpy as np
from gym_malware.envs.utils import interface, pefeatures

from gym_malware.envs.controls import manipulate2 as manipulate
ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

# change this to function to the AV engine to attack
# function should be of the form
# def label_function( bytez ):
#    # returns 0.0 if benign else 1.0 if malware
label_function = interface.get_label_local


class MalwareTableEnv(gym.Env):
    metadata = {'render.modes': ['human']}
    
    def __init__(self, sha256list,test, random_sample=False, maxturns=3, output_path='evaded/table/', cache=False):
        self.cache = cache
        self.available_sha256 = sha256list
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.maxturns = maxturns
        self.feature_extractor = pefeatures.PEFeatureExtractor()
        self.random_sample = random_sample
        self.sample_iteration_index = 0
        self.test = test
        self.count_turns = 0
        self.count_episodes = -1
        self.count_won_episodes = 0
        print("Num. of samples:", len(sha256list))
        # create actions table
        self.actions_table = dict.fromkeys(manipulate.ACTION_TABLE.keys(), 0.0)

        self.actions_table_normalized = dict.fromkeys(manipulate.ACTION_TABLE.keys(), 0.0)
        if test:
            # load table
            self._load_actions_table('actions_table.csv')

        self.output_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), output_path)
        if not os.path.exists(output_path):
            os.makedirs(output_path)

        self.history = OrderedDict()

        self.samples = {}
        if self.cache:
            for sha256 in self.available_sha256:
                try:
                    self.samples[sha256] = interface.fetch_file(self.sha256)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

        self._reset()

    def unique(actions):
        return list(set(actions))

    def _table_update(self, reward, turns, actions):
        # print("reward:", reward, "\t| value:", value) # DEBUG INSTRUCTION
        if reward == 10:
            value = 1
        else:
            value = 0
        print(actions)
        print(type(actions))
        for action in list(set(actions)):
            self.actions_table[action] += value
        
        self.actions_table_normalized = self.actions_table.copy()
        for key in self.actions_table_normalized:
            self.actions_table_normalized[key] = self.actions_table_normalized[key] / self.count_won_episodes
        print("table:", self.actions_table) # DEBUG INSTRUCTION
        print("table normalized:", self.actions_table_normalized) # DEBUG INSTRUCTION
        self._save_actions_table('actions_table.csv')

    def _load_actions_table(self, filename):
        table_file = open(filename, 'r')
        csv_reader = csv.reader(table_file)
        for row in csv_reader:
            self.actions_table_normalized[row[0]] = float(row[1])

    def _save_actions_table(self, filename):
        table_file = open(filename, 'w')
        csv_writer = csv.writer(table_file)
        for key in self.actions_table_normalized:
            csv_writer.writerow([key, self.actions_table_normalized[key]])
        table_file.close()

    def _step(self, action_index):
        self.turns += 1
        self._take_action(action_index) # update self.bytez
        self.count_turns += 1
        # get reward
        try:
            self.label = label_function(self.bytez)
        except interface.ClassificationFailure:
            print("Failed to classify file")
            episode_over = True
        else:
            self.observation_space = self.feature_extractor.extract(self.bytez)
            if self.label == 0:
                # we win!
                reward = 10.0 # !! a strong reward
                self.count_won_episodes += 1
                # aggiornamento della tabella delle potenzialita' delle azioni nel caso di vittoria
                if not self.test:
                    self._table_update(reward, self.turns, self.history[self.sha256]['actions'])

                episode_over = True
                self.history[self.sha256]['evaded'] = True

                # store sample to output directory
                m = hashlib.sha256()
                m.update( self.bytez )
                sha256 = m.hexdigest()
                self.history[self.sha256]['evaded_sha256'] = sha256
                
                with open( os.path.join( self.output_path, sha256), 'wb') as outfile:
                    outfile.write( self.bytez )
            elif self.turns >= self.maxturns:
                # out of turns :(
                reward = 0

                # aggiornamento della tabella delle potenzialita' delle azioni nel caso di sconfitta
                #self._table_update(reward, self.turns, self.history[self.sha256]['actions'])
                episode_over = True

            else:
                # utilizzo della tabella delle potenzialita' per determinare un reward intermedio
                reward = self.actions_table_normalized[ACTION_LOOKUP[action_index]]
                print("intermediate reward:", reward)
                episode_over = False

        if episode_over:
            print("Episode is over: reward = {}!".format(reward))
            print('-'*20, 'END EPISODE N.{}'.format(self.count_episodes), '-'*20)
            # diego: si scrive su file al termine dell'episodio lo storico del sample
            #with open("history-malware_table_env.txt", 'a') as histories_file:
            #  histories_file.write(str(self.history[self.sha256]) + "\n")

        return self.observation_space, reward, episode_over, {}

    def _take_action(self, action_index):
        assert action_index < len(ACTION_LOOKUP)
        action = ACTION_LOOKUP[action_index]
        print(action)
        self.history[self.sha256]['actions'].append(action)
        self.bytez = bytes( manipulate.modify_without_breaking(self.bytez, [action]))

    def _reset(self):
        self.turns = 0
        self.count_episodes += 1
        while True:
            # get the new environment
            if self.random_sample:
                self.sha256 = random.choice(self.available_sha256)
            else: # draw a sample at random
                self.sha256 = self.available_sha256[ self.sample_iteration_index % len(self.available_sha256) ]
                self.sample_iteration_index += 1
            
            self.history[self.sha256] = {'actions': [], 'evaded': False}
            if self.cache:
                self.bytez = self.samples[self.sha256]
            else:
                try:
                    self.bytez = interface.fetch_file(self.sha256)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

            original_label = label_function(self.bytez)            
            if original_label == 0:
                # skip this one, it's already benign, and the agent will learn nothing
                continue

            print("new sha256: {}".format(self.sha256))                
            print('-'*20, 'START EPISODE N.{}'.format(self.count_episodes), '-'*20)
            self.observation_space = self.feature_extractor.extract(self.bytez)

            break  # we're done here

        return np.asarray(self.observation_space)

    def _render(self, mode='human', close=False):
        pass