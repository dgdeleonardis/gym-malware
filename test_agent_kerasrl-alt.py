# imports
import sys
import numpy as np

from gym.envs.registration import register
import gym
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

from keras.optimizers import Adam, SGD, RMSprop

from train_agent_kerasrl import train_dqn_model, generate_dense_model

# consts
ENV_NAME = sys.argv[1]
TEST_NAME = sys.argv[2]
layers = [1024, 256]
rounds = 100

if __name__ == '__main__':
    # 0. training del modello
    print('ENV NAME:', ENV_NAME)

    env = gym.make(ENV_NAME)
    nb_actions = env.action_space.n
    window_length = 1

    #generate a policy model
    model = generate_dense_model(
        (window_length,) + env.observation_space.shape,
        layers,
        nb_actions
    )

    policy = BoltzmannQPolicy()

    memory = SequentialMemory(
        limit=32, 
        ignore_episode_boundaries=False,
        window_length=window_length
    )

    agent = DQNAgent(
        model=model,
        nb_actions=nb_actions,
        memory=memory,
        nb_steps_warmup=16,
        enable_double_dqn=True,
        enable_dueling_network=True,
        dueling_type='avg',
        target_model_update=1e-2,
        policy=policy,
        batch_size=16
    )

    # 1. training
    agent.compile(RMSprop(lr=1e-3), metrics=['mae'])
    agent.fit(
        env,
        nb_steps=rounds,
        visualize=False,
        verbose=2
    )

    history_train = env.history

    # 2. test
    test_env = gym.make(TEST_NAME)
    agent.test(test_env, nb_episodes=10, visualize=False)
    history_test = test_env.history
    print(len(history_test))

    
