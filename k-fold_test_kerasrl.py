# imports
# generic imports
import sys
import os
import numpy as np
import pickle
from datetime import datetime, date
import random
from consts import TEST_SETTINGS, ROUNDS, REPEATS, K, WINDOW_LENGTH, LAYERS
# gym-malware imports
from gym_malware import MAXTURNS, sha256 as dataset
from train_agent_kerasrl import generate_dense_model
# gym imports
from gym.envs.registration import register
import gym
# keras imports
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

from keras.optimizers import RMSprop
# -----------------------------------------------------------------------------
# si determina dato l'input dell'utente su quale environment lavorare
env_name = sys.argv[1]
if env_name in TEST_SETTINGS:
    ENV_SETTING = TEST_SETTINGS[env_name]
else:
    print("Env not defined!")
    exit()

# creazione directory per salvataggio dei dati e modelli
test_date = date.today()
TEST_DIR = '{}-{}'.format(env_name, test_date)
os.mkdir(os.path.join('test', TEST_DIR))


# funzione per salvataggio delle history
def save_history(history, filename, dir, is_test):
    if is_test:
        path = os.path.join(dir, 'test_histories')
    else:
        path = os.path.join(dir, 'train_histories')
    history_file = open(os.path.join(path, filename) + '.pickle', 'wb')
    pickle.dump(history, history_file, pickle.HIGHEST_PROTOCOL)

# funzione per la creazione del DQNAgent
def create_DQNAgent(env_id):
    env = gym.make(env_id)
    nb_actions = env.action_space.n
    # generate a model
    model = generate_dense_model(
        (WINDOW_LENGTH,) + env.observation_space.shape,
        LAYERS,
        nb_actions
    )
    # create policy
    policy = BoltzmannQPolicy()
    # create memory
    memory = SequentialMemory(
        limit=32,
        ignore_episode_boundaries=False,
        window_length=WINDOW_LENGTH
    )
    # create DQNAgent
    agent = DQNAgent(
        model=model,
        nb_actions=nb_actions,
        memory=memory,
        nb_steps_warmup=16,
        enable_double_dqn=True,
        enable_dueling_network=True,
        dueling_type='avg',
        target_model_update=1e-2,
        policy=policy,
        batch_size=16,
    )
    return agent, env

if __name__ == '__main__':
    for i in range(0, REPEATS):
        print('-'*20, 'START n.{} CYCLE'.format(i), '-'*20)
        path = os.path.join('test', TEST_DIR, str(i))
        os.mkdir(path)
        os.mkdir(os.path.join(path, 'models'))
        os.mkdir(os.path.join(path, 'train_histories'))
        os.mkdir(os.path.join(path, 'test_histories'))
        # prendo il dataset  e lo divido in K sottoinsiemi disgiunti
        aux_list = dataset.copy()
        folds = [[] for _ in range(0, K)]
        
        i = 0
        while len(aux_list) != 0:
            entry = random.choice(aux_list)
            folds[i].append(entry)
            aux_list.remove(entry)
            i = (i + 1) % K
        
        # eseguo la K-fold crossvalidation
        arr = [*range(0, K)]
        while len(arr) != 0:
            test_index = random.choice(arr)
            arr.remove(test_index)
            training_indexs = [x for x in range(0, K) if x != test_index]
            
            test_set = folds[test_index]
            training_set = []
            for i in training_indexs:
                training_set.extend(folds[i])

            # registro l'env
            env_id = ENV_SETTING['id'].format(test_index)
            test_env_id = ENV_SETTING['test_id'].format(test_index)
            register(
                id=env_id,
                entry_point=ENV_SETTING["entry_point"],
                kwargs={
                    'test': False,
                    'random_sample': True,
                    'maxturns': MAXTURNS,
                    'sha256list': training_set
                }
            )
            register(
                id=test_env_id,
                entry_point=ENV_SETTING['entry_point'],
                kwargs={
                    'test': True,
                    'random_sample': True,
                    'maxturns': MAXTURNS,
                    'sha256list': test_set
                }
            )

            # creazione DQNAgent
            agent, env = create_DQNAgent(env_id)
            agent.compile(RMSprop(lr=1e-3), metrics=['mae'])
            # training 
            print('-'*20,'START TRAINING', '-'*20)
            print('Starting training with {} of {} environment'.format(ROUNDS, ENV_SETTING['id'].format('')))
            agent.fit(
                env,
                nb_steps=ROUNDS,
                visualize=False,
                verbose=2
            )
            print('-'*20,'END TRAINING', '-'*20)
            
            # salvataggio dati training
            print('Saving training data...')
            history_train = env.history
            save_history(history_train,
                'training_history-{}-{}'.format(env_id, test_date),
                path,
                False
            )
            print('End saving training data')

            # test
            print('-'*20, "START TEST", '-'*20)
            print('Starting tessto with {} of {} environment'.format(ROUNDS, ENV_SETTING['id'].format('')))
            test_env = gym.make(test_env_id)
            agent.test(test_env, nb_episodes=len(test_set), visualize=False)
            print('-'*20,'END TEST', '-'*20)

            # salvataggio dati test
            print('Saving test data...')
            history_test = test_env.history
            save_history(history_test,
                'test_history-{}-{}'.format(test_env_id, test_date),
                path,
                True
            )
            print('End saving test data')

            # salvataggio modello
            agent.save_weights(os.path.join(path, 'models', ENV_SETTING['model_filename'].format(datetime.now().strftime("%d%m%Y-%H:%M:%S"))))
        print('-'*20, 'END n.{} CYCLE'.format(i), '-'*20)
    print('K-Fold test finished!')